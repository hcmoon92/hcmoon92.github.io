# 정보 엔트로피 (Entropy)

---

## 1. 정보 엔트로피 정의
- **정보 엔트로피**는 불확실성을 수학적으로 표현한 개념으로, 특정 메시지를 통해 얻을 수 있는 정보량을 나타냅니다.
- 정보이론에서 **Shannon 엔트로피**로 정의되며, 정보의 평균적인 불확실성을 측정합니다.

---

## 2. 엔트로피 계산 공식
엔트로피 \( H(X) \)는 아래와 같이 정의됩니다:
\[
H(X) = - \sum_{i=1}^{n} P(x_i) \log_2 P(x_i)
\]
- \( P(x_i) \): 사건 \( x_i \)의 발생 확률.
- \( \log_2 \): 이진 로그로, 정보량을 비트(bit) 단위로 측정.

---

## 3. 엔트로피의 물리적 의미
- **높은 엔트로피**: 데이터가 예측 불가능하거나 무작위성이 클 때.
- **낮은 엔트로피**: 데이터가 예측 가능하고 규칙적일 때.

---

## 4. 엔트로피의 응용
### 4.1 데이터 압축
- 엔트로피는 데이터 압축의 이론적 한계를 정의합니다.
- 이상적인 압축률은 엔트로피값에 의해 제한됩니다.

### 4.2 암호화
- 정보 엔트로피는 암호화 시스템의 보안 강도를 평가하는 데 사용됩니다.
- 높은 엔트로피는 더 강력한 암호를 의미합니다.

### 4.3 통신
- 엔트로피는 통신 채널의 용량 계산에 사용됩니다.

---

## 5. 예제
### 5.1 예제 1: 공정한 동전 던지기
- \( P(앞면) = P(뒷면) = 0.5 \)
- 엔트로피:
\[
H(X) = -[0.5 \log_2 0.5 + 0.5 \log_2 0.5] = 1 \text{ 비트}
\]

### 5.2 예제 2: 불공정한 동전
- \( P(앞면) = 0.9, P(뒷면) = 0.1 \)
- 엔트로피:
\[
H(X) = -[0.9 \log_2 0.9 + 0.1 \log_2 0.1] \approx 0.469 \text{ 비트}
\]

---

## 6. 한계와 주의사항
- 엔트로피는 평균적인 정보를 측정하므로 특정 사건에 대한 정보량을 나타내지 않습니다.
- 확률 분포를 정확히 알아야 정확한 엔트로피를 계산할 수 있습니다.

---

## 참고
- Shannon, C. E. (1948). "A Mathematical Theory of Communication"
- 정보 엔트로피는 데이터 과학, 통신 이론, 인공지능 등 여러 분야에서 중요한 역할을 합니다.
